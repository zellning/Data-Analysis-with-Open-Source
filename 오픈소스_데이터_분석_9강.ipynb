{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zellning/Data-Analysis-with-Open-Source/blob/main/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D_9%EA%B0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIybDN9Js4C0"
      },
      "source": [
        "# 오픈소스 기반 데이터 분석 9강"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 런타임 -> 런타임 유형 변경 (GPU 로 설정)"
      ],
      "metadata": {
        "id": "FJAZk_HdUFBk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gz6LZw7UCyv"
      },
      "outputs": [],
      "source": [
        "# 한글 처리를 위한 matplotlib 설정 (1)\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoE-8V38UCyy"
      },
      "source": [
        "- 런타임 -> 세션 다시 시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIGHlLjNUCyz"
      },
      "outputs": [],
      "source": [
        "# 한글 처리를 위한 matplotlib 설정 (2)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 통계적 분석 방법론"
      ],
      "metadata": {
        "id": "aHPEN6rbF5Vv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U80AIz_OUCy0"
      },
      "source": [
        "## 9-1 아이리스 데이터셋 특성 간 상관관계 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjGh5Oo7d2Qp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## 아이리스 데이터셋 로드\n",
        "df = sns.load_dataset('iris')\n",
        "\n",
        "## 상관계수 행렬 계산 및 출력\n",
        "correlation = df.iloc[:, : -1].corr()\n",
        "print(\"상관계수 행렬:\")\n",
        "print(correlation)\n",
        "\n",
        "## 상관관계 히트맵 시각화\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "plt.title('특성 간 상관관계')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o4xJ7qcUCy2"
      },
      "source": [
        "## 9-2 아이리스 데이터셋 다중 선형 회귀 모델 구축 및 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njapJSVgd4u7"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## 아이리스 데이터셋 로드\n",
        "df = sns.load_dataset('iris')\n",
        "\n",
        "## 다중 선형 회귀 모델 생성 및 학습\n",
        "## 'petal_width'를 종속 변수로 하고 다른 특성들(sepal_length, sepal_width, petal_length)을 독립 변수로 사용\n",
        "model = smf.ols(\n",
        "    formula = 'petal_width ~ sepal_length + sepal_width + petal_length',\n",
        "    data = df\n",
        ").fit()\n",
        "\n",
        "## 모델 요약 결과 출력\n",
        "print(model.summary())\n",
        "\n",
        "## 실제 값 vs 예측 값 시각화\n",
        "plt.figure(figsize=(8, 6))\n",
        "predictions = model.predict(df) ## 모델 예측값 계산\n",
        "plt.scatter(df['petal_width'], predictions, alpha=0.7) ## 실제 값과 예측 값 산점도\n",
        "plt.plot([df['petal_width'].min(), df['petal_width'].max()],\n",
        "         [df['petal_width'].min(), df['petal_width'].max()], 'k--', lw=2) ## 이상적인 예측 라인 (y=x)\n",
        "plt.xlabel('Actual Values', fontsize=12)\n",
        "plt.ylabel('Predicted Values', fontsize=12)\n",
        "plt.title('Actual vs Predicted Values', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWwYnTFRUCy4"
      },
      "source": [
        "## 9-3 아이리스 데이터셋 t-검정 수행\n",
        "t검정 : 한개와 한개 즉 2개 사이의 관계를 봄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huMMw26id8_V"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from statsmodels.stats.weightstats import ttest_ind\n",
        "\n",
        "## 아이리스 데이터셋 로드\n",
        "df = sns.load_dataset('iris')\n",
        "\n",
        "## 비교할 두 품종의 꽃받침 길이 데이터 추출\n",
        "setosa = df[df['species'] == 'setosa']['sepal_length']\n",
        "versicolor = df[df['species'] == 'versicolor']['sepal_length']\n",
        "\n",
        "## 독립 표본 t-검정 수행\n",
        "## 두 그룹(setosa, versicolor)의 'sepal_length' 평균이 통계적으로 유의미한 차이가 있는지 검정\n",
        "t_stat, p_val, df_dof = ttest_ind(setosa, versicolor)\n",
        "\n",
        "## t-통계량과 p-값 출력\n",
        "print(f\"t-통계량: {t_stat:.4f}, p-값: {p_val:.4f}\")\n",
        "\n",
        "## 검정 결과 해석\n",
        "if p_val < 0.05:\n",
        "    print(\"귀무가설 기각: 두 품종의 꽃받침 길이에 유의한 차이가 있다\")\n",
        "else:\n",
        "    print(\"귀무가설 채택: 두 품종의 꽃받침 길이에 유의한 차이가 없다\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSmgobYwUCy6"
      },
      "source": [
        "## 9-4 아이리스 데이터셋 일원 분산 분석 (ANOVA)\n",
        "분석해야하는것이 3개이상 (즉 여러개) 일 때 ANOVA를 이용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "affMANqLd_9r"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "## 아이리스 데이터셋 로드\n",
        "df = sns.load_dataset('iris')\n",
        "\n",
        "## 일원 분산 분석(ANOVA) 모델 생성 및 학습\n",
        "## 'sepal_length'에 대한 'species'의 영향 분석\n",
        "model = ols('sepal_length ~ C(species)', data = df).fit()\n",
        "\n",
        "## ANOVA 테이블 생성 및 출력\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print(\"ANOVA 결과:\")\n",
        "print(anova_table)\n",
        "\n",
        "## p-값을 이용한 결과 해석\n",
        "p_val = anova_table['PR(>F)'][0]\n",
        "if p_val < 0.05:\n",
        "    print(\"\\n결과: 귀무가설 기각 (p < 0.05)\")\n",
        "    print(\"세 품종 간 꽃받침 길이에 통계적으로 유의미한 차이가 존재합니다.\")\n",
        "else:\n",
        "    print(\"\\n결과: 귀무가설 채택 (p >= 0.05)\")\n",
        "    print(\"세 품종 간 꽃받침 길이에 유의미한 차이가 없습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9pvZEPNUCy7"
      },
      "source": [
        "## 9-5 시계열 데이터 분석 (항공 승객 수)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8IRj7JVeHzT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "## seaborn에서 flights 데이터셋 로드\n",
        "flights = sns.load_dataset('flights')\n",
        "\n",
        "## 날짜 형식 변환 및 인덱스 설정\n",
        "## 'year'와 'month' 컬럼을 이용하여 'date' 컬럼 생성 후 인덱스로 설정\n",
        "flights['date'] = pd.to_datetime(\n",
        "    flights['year'].astype(str) + '-' +\n",
        "    flights['month'].astype(str) + '-01'\n",
        ")\n",
        "flights = flights.set_index('date').sort_index()\n",
        "\n",
        "## 시계열 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(flights['passengers'])\n",
        "plt.title('월별 항공 승객 수 (1949-1960)')\n",
        "plt.xlabel('연도')\n",
        "plt.ylabel('승객 수')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "## 시계열 분해 (추세, 계절성, 잔차)\n",
        "## 시계열 데이터를 추세(Trend), 계절성(Seasonal), 잔차(Residual)로 분해\n",
        "decomposition = seasonal_decompose(flights['passengers'], model = 'multiplicative', period=12)\n",
        "\n",
        "fig = decomposition.plot()\n",
        "fig.set_size_inches(12, 8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기계학습 기반 분석 방법론"
      ],
      "metadata": {
        "id": "wp0gikjMF-sh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt6r6kQQUCy8"
      },
      "source": [
        "## 9-6 여러 분류 모델 교차 검증 및 성능 비교"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RrwLpGueKFj"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "## 아이리스 데이터셋 로드\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "## 다양한 분류 모델 정의\n",
        "models = {\n",
        "    '로지스틱 회귀' : LogisticRegression(),\n",
        "    '결정 트리' : DecisionTreeClassifier(),\n",
        "    '랜덤 포레스트' : RandomForestClassifier(),\n",
        "    '서포트 백터 머신' : SVC(), #벡터머신 : 한동안 지도학습의 최강자로 사용되던 대표적인 알고리즘\n",
        "    'K-최근접 이웃' : KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "## 각 모델별 교차 검증 수행 및 결과 출력\n",
        "for name, model in models.items():\n",
        "    # cross_val_score를 이용하여 학습 및 성능 평가를 수행\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy') #교차검증할 떄 자주사용하는 형태\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  평균 정확도 = {scores.mean():.4f} (±{scores.std():.4f})\")\n",
        "    print(f\"  개별 폴드 점수 = {[round(score, 4) for score in scores]}\")\n",
        "    print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crKpxhGUCy9"
      },
      "source": [
        "## 9-7 여러 회귀 모델 교차 검증 및 성능 비교\n",
        "회귀 : 앞으로의 미래의 어떻게 값이 변하는 것인지 클래스가 정해져 있는게 아니라 어떤 값을 예측하는 것(주식가격, 부동산 가격 등)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyxkw-WyeRqV"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "## 캘리포니아 주택 데이터셋 로드\n",
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target\n",
        "\n",
        "## 다양한 회귀 모델 정의\n",
        "models = {\n",
        "    '선형 회귀' : LinearRegression(),\n",
        "    '릿지 회귀' : Ridge(),\n",
        "    '라쏘 회귀' : Lasso(),\n",
        "    '결정 트리 회귀' : DecisionTreeRegressor(),\n",
        "    '랜덤 포레스트 회귀' : RandomForestRegressor() #시간이 좀 걸림\n",
        "}\n",
        "\n",
        "## 각 모델별 교차 검증 수행 및 RMSE 출력\n",
        "for name, model in models.items():\n",
        "    ## 5-폴드 교차 검증으로 MSE(평균 제곱 오차) 계산\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error') # scoring='neg_mean_squared_error' : 회귀는 맞았다 틀렸다를 구할 수 없음, 얼만큼 오차가 발생하는지 평균적으로 오차의 발생 정도(오차가 적을 수록 좋은 모델이다)\n",
        "    rmse = (-scores.mean()) ** 0.5 ## MSE를 RMSE로 변환\n",
        "    print(f\"{name}: RMSE = {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKa9bkdTUCy-"
      },
      "source": [
        "## 9-8 아이리스 데이터셋 군집화 및 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haLpOuw4ejqm"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "## 아이리스 데이터셋 로드\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "## K-평균 군집화 수행\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "## PCA를 이용한 차원 축소 (2차원) #PCA = 주성분분석\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "## 군집화 결과 및 실제 클래스 시각화\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "## K-평균 군집화 결과 시각화\n",
        "plt.subplot(1, 2, 1)\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
        "plt.title('K-평균 군집화 결과 (PCA)')\n",
        "plt.colorbar(scatter, label='군집')\n",
        "\n",
        "## 실제 클래스 시각화\n",
        "plt.subplot(1, 2, 2)\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
        "plt.title('실제 클래스 (PCA)')\n",
        "plt.colorbar(scatter, label='클래스')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 딥러닝 기반 분석 방법론"
      ],
      "metadata": {
        "id": "BMsITgH_Gz3U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD_FiXMyUCy_"
      },
      "source": [
        "## 9-9 아이리스 데이터셋 신경망 모델 구축 및 평가\n",
        "\n",
        "결과값을 보면 기존의 기계 학습보다 설명력은 떨어지지만(왜 이렇게 나왔는지 알 수 없음), 강력한 예측력을 가진다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwKEMWXtenNc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf #tensorflow : 실행엔진\n",
        "#신경망에서 주로사용되는 컴포넌트 : keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import set_random_seed\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# 랜덤 시드 값 설정\n",
        "set_random_seed(42)\n",
        "\n",
        "## 아이리스 데이터셋 로드 및 전처리\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = tf.keras.utils.to_categorical(iris.target, num_classes=3) ## 타겟 변수를 원-핫 인코딩\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "## 특성 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "## 신경망 모델 구축\n",
        "model = Sequential([\n",
        "    Dense(10, activation = 'relu', input_shape =(4,)), #첫번째 은닉층 10개\n",
        "    Dense(8, activation = 'relu'),                    #두번째 은닉층 8개\n",
        "    Dense(3, activation = 'softmax')                  #출력층 은닉층 3개 클래스 분류\n",
        "])\n",
        "# 이 모델은 다층 퍼셉트론(MLP) 신경망으로, 3개의 완전 연결층(2개의 은닉층과 1개의 출력층)으로 구성\n",
        "# 입력 특징은 총 4개, 각 층의 노드(뉴런) 수는 순서대로 10개, 8개, 3개\n",
        "\n",
        "\n",
        "## 모델 컴파일\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## 모델 구조 요약 출력\n",
        "model.summary()\n",
        "\n",
        "## 모델 학습\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=16,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=0) ## 학습 과정 출력 비활성화\n",
        "\n",
        "## 모델 평가\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'테스트 정확도: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUfudLDbUCzA"
      },
      "source": [
        "## 9-10 MNIST 데이터셋 CNN 모델 구축 및 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy_ON7J6eqWL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import set_random_seed\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# 랜덤 시드 값 설정\n",
        "set_random_seed(42)\n",
        "\n",
        "## MNIST 데이터셋 로드 및 전처리\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "## 이미지 데이터 형태 변환 및 정규화\n",
        "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "## 타겟 레이블을 원-핫 인코딩\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "## CNN 모델 구축\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation = 'relu', input_shape = (28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)), ##첫번째 플링층\n",
        "    Conv2D(64, kernel_size=(3, 3), activation = 'relu'), ##두번째 플링층\n",
        "    MaxPooling2D(pool_size=(2, 2)), ##두번째 플링층\n",
        "    Flatten(), ##1차원으로 평탄화\n",
        "    Dense(128, activation='relu'), ##완전연결층\n",
        "    Dense(10, activation='softmax') ##출력층(10개 클래스 분류)\n",
        "])\n",
        "# 이 모델은 이미지 처리에 특화된 합성곱 신경망(CNN)\n",
        "# 입력 이미지의 형태는 **(높이, 너비, 채널)**이며, 28x28 흑백 이미지의 이므로 (28, 28, 1)\n",
        "# **두 계층의 합성곱 층**, **평탄화 후 완전 연결층**, 그리고 **출력층**으로 구성\n",
        "\n",
        "\n",
        "## 모델 컴파일\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## 모델 학습\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=5, validation_split=0.1, verbose=1)\n",
        "\n",
        "## 모델 평가\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'테스트 정확도: {score[1]:.4f}')\n",
        "\n",
        "## 예측 결과 시각화\n",
        "predictions = model.predict(X_test[:5])\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        "    predicted_label = tf.argmax(predictions[i]).numpy()\n",
        "    true_label = tf.argmax(y_test[i]).numpy()\n",
        "    ax.set_title(f'예측: {predicted_label}, 실제: {true_label}')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ODiLPX7UCzB"
      },
      "source": [
        "## 9-11 IMDB 영화 리뷰 감성 분석 (LSTM 모델)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SCXwUloevVn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import set_random_seed\n",
        "\n",
        "# 랜덤 시드 값 설정\n",
        "set_random_seed(42)\n",
        "\n",
        "## 데이터셋 로드 및 전처리\n",
        "vocab_size = 10000 ## 사용할 단어의 최대 개수\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size) ## IMDB 데이터셋 로드\n",
        "\n",
        "# 학습 데이터 크기 축소\n",
        "X_train = X_train[:len(X_train) // 20]\n",
        "y_trian = y_train[:len(y_train) // 20]\n",
        "\n",
        "max_length = 200 ## 시퀀스(리뷰)의 최대 길이\n",
        "X_train = pad_sequences(X_train, maxlen=max_length) ## 시퀀스 길이 맞추기 (패딩)\n",
        "X_test = pad_sequences(X_test, maxlen=max_length)\n",
        "\n",
        "## LSTM 모델 구축\n",
        "embedding_dim = 128 ## 임베딩 벡터 차원\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length), ##임베딩 층\n",
        "    LSTM(128, dropout = 0.2, recurrent_dropout=0.2), ##LSTM 층\n",
        "    Dense(1, activation = 'sigmoid') ##이진분류를 위한 출력 (긍정/부정)\n",
        "])\n",
        "# 이 모델은 시퀀스 데이터(예: 텍스트) 처리에 특화된 순환 신경망(Recurrent Neural Network, RNN)의 한 종류인 LSTM을 사용\n",
        "# 주요 구성은 임베딩 층, LSTM 층, 그리고 출력층으로 구성\n",
        "\n",
        "\n",
        "## 모델 컴파일\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## 모델 학습\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=1, validation_split=0.2)\n",
        "\n",
        "## 모델 평가\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'테스트 정확도: {score[1]:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}